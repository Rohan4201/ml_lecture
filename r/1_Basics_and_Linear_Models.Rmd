---
title: "Basics and Linear Models"
subtitle: "WORK IN PROGRESS"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE,
                      message = FALSE, 
                      fig.height = 5,
                      fig.width = 6)
```

# Basics

Before heading into linear regression and the generalized linear model, we introduce some basic notation.

## Data organization

*Structured* data is organized as one or multiple tables. Each row of a table represents an *observation*, each column $X$ a *variable* and each cell a *value*. A value in column $X$ can be viewed as a realization of the *random variable* $X$. 

Examples of *unstructured* data are images, text, audio or video data. We will deal with structured data only.

Throughout this lecture, we will consider the following two data sets:

- `diamonds`: Diamonds prices along with the four "C"-variables: Carat, Color, Cut, and Clarity. Each observation/row represents a diamond.

- `dataCar`: Insurance claim data on vehicle insurance policies from 2004-2005. Some variables like `gender` describe the policy holder, some variables (e..g `veh_age`) the vehicle and some variables carry infos on eventual claims. Each row represents a policy.

### Example: diamonds

Let's look at the first six observations of the diamonds data set.

```{r}
library(ggplot2) # Contains "diamonds"
head(diamonds)
```

The data set is neatly structured. It seems to be sorted by price.

## Data types

We distinguish variables by their data type.

- **numerical:** The values of a numerical variable are numbers. Taking sums, means and differences makes sense. Examples: house prices, insurance claim frequencies, blood pressure.
- **categorical:** The values of a categorical variable are categories, e.g. house types, claim types or colors. Depending on whether categories follow a natural order, we talk of *ordered* or *unordered* categoricals. Categories can be encoded by numbers. That does not make the variable numeric.
- **binary:**: A binary variable just takes two values (male/female, yes/no, ...) that can be represented by 1/0. It counts as both numeric and categorical.

Data types are important in determining suitable analysis methods.

### Example: diamonds

In the `diamonds` data set, we will consider the numeric variables `price` and `carat` and the following *ordered* categoricals:

- `color` with ordered categories D < E < F < G < H < I < J,
- `cut` with ordered categories Fair < Good < Very Good < Premium < Ideal, and
- `clarity` with ordered categories I1 < SI2 < SI1 < VS2 < VS1 < VVS2 < VVS1 < IF.

There are no unordered categoricals or binary variables in this data set.

## Descriptive analysis

Statistical modeling always starts with a descriptive analysis of the data. This typically involves a numeric and/or graphical summary of each variable and the most relevant variable pairs, e.g. 

- for numeric variables: mean, standard deviation, quartiles, minimum and maximum; boxplots, mean plots,
- for categorical variables: absolute or relative counts; barplots,
- for binary variables: like a categorical variable or simply the mean,
- for important pairs of numeric variables: correlations; scatter plots,
- the important pairs of categorical variables: frequency tables; mosaic plots,
- for important pairs of categorical *and* numerical variables: stratified means; mean plots, boxplots .

The more time we invest in the descriptive analysis, the more we learn about the data. This is essential for successful modeling. Besides getting used to the data set, we might 

- identify data errors and problematic outliers (uncommonly small or large values), 
- find join keys to combine multiple data sources, 
- detect variables with high number of missing values,
- detect variables with no or almost no information (e.g. all values the same),
- **learn how the data is structured** (what *is* a row? do we deal with time series? are rows grouped to clusters etc),
- ...

During or after the descriptive analysis, we usually do the *data preprocessing* for the modeling task.

### Example: diamonds

Let's summarize the diamonds data set.

```{r}
library(ggplot2)

# Data size
nrow(diamonds)  # 53940

# Univariate description
summary(diamonds[, c("price", "carat", "color", "cut", "clarity")])

ggplot(diamonds, aes(x = price)) +
  geom_histogram(fill = "orange") +
  ggtitle("Distribution of 'price'")

ggplot(diamonds, aes(x = carat)) +
  geom_histogram(fill = "orange") +
  ggtitle("Distribution of 'carat'")

ggplot(diamonds, aes(x = color)) +
  geom_bar(fill = "orange") +
  ggtitle("Distribution of 'color'")

ggplot(diamonds, aes(x = cut)) +
  geom_bar(fill = "orange") +
  ggtitle("Distribution of 'cut'")

ggplot(diamonds, aes(x = clarity)) +
  geom_bar(fill = "orange") +
  ggtitle("Distribution of 'clarity'")

# Selected bivariate descriptions
ggplot(diamonds, aes(y = price, x = carat)) +
  geom_point(alpha = 0.2, shape = ".") +
  ggtitle("Price against carat")

ggplot(diamonds, aes(y = price, x = color)) +
  geom_boxplot(fill = "orange", varwidth = TRUE) +
  ggtitle("Price against color")

ggplot(diamonds, aes(y = price, x = cut)) +
  geom_boxplot(fill = "orange", varwidth = TRUE) +
  ggtitle("Price against cut")

ggplot(diamonds, aes(y = price, x = clarity)) +
  geom_boxplot(fill = "orange", varwidth = TRUE) +
  ggtitle("Price against clarity")

```

**Comments**

- There are 53'940 observations.
- The avarage or mean price is 3933 USD. About half of the diamonds cost less/more than the median of 2401 USD and about half of them cost between 950 USD (first quartile) and 5324 USD (third quartile). The cheapest diamond in the data set has a price of 326 USD, the most expensive one is 18'823 USD.
- Prices and carats show a *right-skewed* distribution with a few very large values.
- The relationship between price and carat seems quite strong and positive, which makes sense.
- The worst categories of `color`, `cut`, and `clarity` are rare.
- Prices tend to be lower for nice colors, nice cuts as well as for nice clarities. This is unintuitive but will be entangled later by our regression models.

## Statistical models

The general modeling task is as follows: we want to approximate a *response variable* $Y$ by a function $f$ of $m$ *covariables* $X_1, \dots, X_m$, i.e.
$$
  Y \approx f(X_1, \dots, X_m).
$$
The function $f$ is unknown and we want to estimate it by $\hat f$ from observed data. 

Note: Think of the response $Y$ and the covariables $X_1, \dots, X_m$ as columns in a dataset.

Normally, we are interested in modeling a specific property of $Y$, usually its expection $E(Y)$ (= theoretic mean). In that case, we can make above approximate relationship more explicit by writing down the *model equation*
$$
  E(Y) = f(X_1, \dots, X_m).
$$

Once found, $\hat f$ serves as our prediction function that can be applied to fresh data. Furthermore, we can investigate the structure of $\hat f$ to gain insights about the relationship between response and covariables: what variables are especially important? how do they influence the response?

**Remark:** Other terms for "response variable" are "output", "target" or "dependent variable". Other terms for "covariable" are "input", "feature", "independent variable" or "predictor".

# Linear Regression

In order to get used to the terms mentioned above, we will look at the mother of all machine learning algorithms: (multiple) linear regression. It was first published by Adrien-Marie Legendre in 1805 [1] and is still very frequently used thanks to its simplicity and interpretability. It further serves as a simple benchmark for more complex algorithms and is the starting point for extensions like the generalized linear model.

## Model equation

The model equation of the linear regression is as follows:
$$
  E(Y) = f(X_1, \dots, X_m) = \beta_0 + \beta_1 X_1 + \cdots + \beta_m X_m.
$$
It relates the covariables $X_1, \dots, X_m$ to the expected response $E(Y)$ by a *linear* formula in the parameters $\beta_0, \dots, \beta_m$. The additive constant $\beta_0$ is called *intercept*. The parameter $\beta_i$ tells us by how much $y$ is expected to change when $X_i$ is increased by 1, **keeping all other covariables fixed** ("Ceteris Paribus"):
$$
  E(Y \mid X_i = x + 1) - E(y \mid X_i = x) = \beta_i (x + 1) - \beta_i x = \beta_i.
$$
The parameter $\beta_i$ is thus called *effect* of $X_i$ on the expected response $E(Y)$.

A linear regression with just one covariable is called a *simple* linear regression with equation $E(Y) = \alpha + \beta X$.

## Least-squares method

The optimal $\hat f$ to estimate $f$ is found by minimizing the sum of squared *prediction errors* resp. *residuals*
$$
  \sum_{i=1}^n e_i^2 = (y_i - \hat y_i)^2.
$$
$y_i$ is the observed value of observation $i$ and $\hat y_i$ its prediction (or *fitted value*)
$$
  \hat y_i = \hat f(\text{Values of covariables of observation } i).
$$

Once the model is fitted, we can use the coefficients $\hat\beta_0, \dots, \hat\beta_m$ to make predictions and to study empirical effects of the covariables on the expected response.

## Example: diamonds

In order to discuss the typical output of a linear regression, we now model diamond prices by their size. The model equation is
$$
  E(\text{price}) = \alpha + \beta \cdot \text{carat}.
$$

```{r}
library(ggplot2)

fit <- lm(price ~ carat, data = diamonds)
summary(fit)
intercept <- coef(fit)[1]
slope <- coef(fit)[2]

# Visualize the regression line
ggplot(diamonds, aes(x = carat, y = price)) + 
  geom_point(alpha = 0.2, shape = ".") +
  coord_cartesian(xlim = c(0, 3), ylim = c(-3000, 20000)) +
  geom_abline(slope = slope, intercept = intercept, color = "orange", size = 1)

# Predictions for diamonds with 0.3 and 1.3 carat?
predict(fit, data.frame(carat = c(0.3, 1.3)))

# By hand
(p1 <- intercept + slope * 0.3)
(p2 <- intercept + slope * 1.3)
(p2 <- p1 + slope * 1)
```

**Comments**

- **Regression coefficients:** The intercept $\alpha$ is estimated by $\hat \alpha = -2256$ and the effect of carat $\beta$ by $\hat \beta = 7756$ USD. This means that a 1 carat increase goes along with an average increase in price of 7756 USD. Similarly, we could say that a 0.1 increase in carat is associated with an increase in the price of 775.6 USD.
- **Regression line:** For a simple linear regression, the estimated regression coefficients $\hat \alpha$ and $\hat \beta$ can be visualized by the regression line. The latter represents the scatterplot as good as possible in the sense that the sum of squared vertical distances from the points to the line are minimal. The $y$-value at $x = 0$ equals $\hat \alpha = -2256$ and the slope of the line is $\hat \beta = 7756$.
- **Predictions:** Model predictions are made by using the fitted model equation $-2256 + 7756 \cdot \text{carat}$. For a diamond of size 0.3 carat, we get 71 USD. For 1.3 carat, we get $-2256 + 1.3 \cdot 7756 \approx 7827$. These values correspond to the value on the regression line.

## Quality of the model

How good is a specific linear regression model? Here, we consider two aspects, namely 

- its predictive performance and 
- how well its assumptions are valid.

### Predictive performance

How precise are the model predictions? I.e. how well do predictions correspond with the observed response? In line with the least-squares approach, this is quantified by the sum of squared prediction errors
$$
  \sum_{i = 1}^n (y_i - \hat y_i)^2
$$
or, equivalently by the *mean-squared-error*
$$
  \text{MSE} = \frac{1}{n}\sum_{i = 1}^n (y_i - \hat y_i)^2.
$$
To quantify the size of the typical prediction error on the same scale as $Y$, we take the square-root of the MSE and get the *root-mean-squared error* (RMSE). 

Besides an *absolute* performance measure like the RMSE, we gain additional insights by studying a relative performance measure like the **R-squared**. It measures the relative decrease in MSE compared to the MSE of the "empty" model
$$
  E(Y) = \alpha
$$
without any covariables. Put differently, the R-squared measures the proportion of variability of $Y$ explained by the covariables.

#### Example: diamonds continued

Let us calculate these performance measures for the simple linear regression above.

```{r}
mse <- function(y, pred) {
  mean((y - pred)^2)
}
(MSE <- mse(diamonds$price, predict(fit, diamonds)))
(RMSE <- sqrt(MSE))

empty_model <- lm(price ~ 1, data = diamonds)
MSE_empty <- mse(diamonds$price, predict(empty_model, diamonds))

# R-squared
(MSE_empty - MSE) / MSE_empty
```

**Comments**

- The RMSE is 1549 USD. This means that residuals (= prediction errors) are typically around 1549 USD.
- The R-squared shows that about 85% of the price variability can be explained by carat.

### Model assumptions

The main assumption of linear regression is that the **model equation is correctly specified** in the sense that predictions are not systematically too high or too small for certain values of the covariables. In a simple regression setting, this means that the points in the scatterplot are located *around* the regression line for all $x$ values. 

Additional assumptions like independence of rows, constant variance of the error term $\varepsilon$ in the equation
$$
  Y = f(X_1, \dots, X_m) + \varepsilon
$$
and normal distribution of $\varepsilon$ guarantee optimality of the least-squares estimator $\hat \beta_0, \dots, \hat\beta_m$ and the correctness of inferential statistics (standard errors, p values, confidence intervals). If all these assumptions are made, we talk of the *normal linear model*.

#### Example: diamonds continued

For small diamonds, the predictions of our simple linear regression are systematically too low. This indicates a wrongly specified model. We will see a bit later how to fix this problem.

## Typical problems

Here, we list some problems that frequently occurs with linear regression. We will only mention them without going into details.

### Missing values

Like many other ML algorithms, linear regression cannot deal with missing values. Rows with missing response can be safely dropped, while missings in covariables should usually be dealt with. The simplest (often too naive) approach is to fill missings with a typical value such as the mean or the most frequent value.

### Outliers

Gross outliers in covariables can distort the result of the linear regression. Do not delete them, but try to reduce their effect by taking logarithms or by using more robust regression techniques. Outliers in the response are often surprisingly unproblematic.

### Overfitting

If too many parameters are used in relation to the number of observations, the resulting model might look good but would not generalize well to new data. This is called overfitting. A small amount of overfitting is unproblematic. Do not fit a model with $m=100$ parameters on a data set with just $n=200$ rows. The resulting model would be garbage. A $n/m$ ratio of 50-100 is usually safe for stable estimation of parameters.

### Collinearity

If the association between two or more covariables is strong, their coefficients are difficult to interpret because the Ceteris Paribus clause is usually unnatural in such situations. E.g. in a house price model, it is unnatural to study the effect of an additional room, keeping living area fixed. This is even more problematic for causally dependent covariables: imagine a model with $x$ and $x^2$ as covariables. It would certainly not make sense to study the effect of $x$ while keeping $x^2$ fixed. 

Strong collinearity can be detected by looking at correlations across regressors. It is mainly a problem when interpreting effects or testing statistical hypotheses. Predictions or other "global" model characteristics like the R-squared are not affected.

## Representation of covariables

Depending on the specific situation, covariables might need to be transformed before using them in a regression:

- **Dummy coding:** Algorithms usually only understand numbers, not categories. Thus, each category of a categorical covariable can be represented by its own "dummy" covariable ("am I in this category yes/no?"). Since exactly one dummy variable has the value 1, one of them is redundant and is dropped from linear regression. Other ML algorithms such as neural nets would require all dummy variable (including the redundant one). In this case we speak of "one-hot-encoding (OHE)".
Note: Rare categories can be lumped together in order to not estimate too many parameters and to reduce overfitting.  
- **Integer coding:** Besides dummy coding, ordinal categorical covariables are sometimes integer encoded for simplicity i.e. each category is represented by an integer number instead a of a binary variable. This assumes (approximate) equally spaced categories.
- **Log-tranformation:** Positive numeric variables can be log-transformed to deal with large outliers and/or to bring it to a relative scale. When talking about logarithms, we mean the *natural* logarithm.
- **Non-linear terms:** Important numeric covariables might be represented by more than just one parameter (= slope) to allow for a more flexible and non-linear associations to the response. Adding a squared term e.g. allows for curvature. Especially flexible are regression splines that can approximate arbitrary smooth relationships at the price of interpretability.
- **Decorrelation:** Strongly related covariables can be decorrelated to reduce negative consequences of collinearity. Instead of e.g. using number of rooms and living area in a house price model, it might help to represent living area by the derived variable "living area per room".
- **Interactions:** As a consequence of the additive model structure, each covariable is assumed to act independently on the response, keeping the other covariables fixed. This is not always realistic. E.g. being flawless might mean more to a large diamond than to a small diamond. In such cases, adding so-called interaction terms might be worth considerung. Since interaction terms complicate the interpretation of the results, they should be used with care and only for important covariables. Adding interaction terms between a covariable $X$ and a categorical covariable $Z$ provides an effect of $X$ for each value of $Z$.

## Logarithms are important

We have already mentioned that (natural) logarithms help to reduce the effects of outliers. They also make effects "relative", which is often very useful for both the response and/or a covariable.

For simplicity, we study this for the simple linear regression.

### Logarithm of covariable

The coefficient $\beta$ of 
If we represent a covariable $X$ by its logarithm, then its effect $\beta$ :

$$
E(Y\mid X = 1.01 \cdot x) - E(Y\mid X = x) = \alpha + \beta \cdot (1.01) \cdot x - \alpha - \beta x = \beta/100.
$$

```{r}
library(ggplot2)

fit <- lm(price ~ log(carat), data = diamonds)
summary(fit)
intercept <- coef(fit)[1]
slope <- coef(fit)[2]

# Visualize the regression line
ggplot(diamonds, aes(x = log(carat), y = price)) + 
  geom_point(alpha = 0.2, shape = ".") +
  coord_cartesian(ylim = c(-3000, 20000)) +
  geom_abline(slope = slope, intercept = intercept, color = "orange", size = 1)

# Predictions for diamonds with 1.3 carat?
predict(fit, data.frame(carat = 1.3))

# By hand
intercept + slope * log(1.3)
```

### Logarithm of response

```{r}
library(ggplot2)

fit <- lm(log(price) ~ carat, data = diamonds)
summary(fit)
intercept <- coef(fit)[1]
slope <- coef(fit)[2]

# Visualize the regression line
ggplot(diamonds, aes(x = carat, y = log(price))) + 
  geom_point(alpha = 0.2, shape = ".") +
  coord_cartesian(xlim = c(0, 3)) +
  geom_abline(slope = slope, intercept = intercept, color = "orange", size = 1)

# Predictions for diamonds with 1.3 carat?
exp(predict(fit, data.frame(carat = 1.3)))

# By hand
exp(intercept + slope * 1.3)
```

### Logarithm of covariable and response

```{r}
library(ggplot2)

fit <- lm(log(price) ~ log(carat), data = diamonds)
summary(fit)
intercept <- coef(fit)[1]
slope <- coef(fit)[2]

# Visualize the regression line
ggplot(diamonds, aes(x = log(carat), y = log(price))) + 
  geom_point(alpha = 0.2, shape = ".") +
  geom_abline(slope = slope, intercept = intercept, color = "orange", size = 1)

# Predictions for diamonds with 1.3 carat?
exp(predict(fit, data.frame(carat = 1.3)))

# By hand
exp(intercept + slope * log(1.3))
```

## Example: diamonds improved

Now, we try to improve our simple linear regression in two ways.

1. We add color, cut and clarity (implicitly as dummy variables).

2. We model logarithmic price and carat. (Remember the scatter plot with logarithmic axes with a relatively strong linear relationship.)

```{r}
dia <- transform(diamonds,
                 color_ = factor(color, ordered = FALSE),
                 cut_ = factor(cut, ordered = FALSE),
                 clarity_ = factor(clarity, ordered = FALSE))
fit <- lm(log(price) ~ log(carat) + color_ + cut_ + clarity_, data = dia)
summary(fit)
```

**Comments**

- Effects of dummy coded variables like color are with respect to the first (= reference) category, which is represented by the intercept. E.g. we can say that average log prices are 0.054 lower for color "E" than for color "D". Worse colors seem to be associated with lower prices. Do you remember the unintuitive descriptive result from earlier above? There we had gained the impression that worse colors tend to higher prices. Taking into account the effects of other variables like carat has switched the sign of the color effect! Studying effects *adjusted* for other variables (resp. keeping all other variables fixed) is often at least as useful than looking at simple descriptive figures.

- Since our model uses log-price as response, we might be interested to translate effects to the original level. This can be done by exponentiating the estimated coefficients or by interpreting the coefficients directly as approximate percentages. We can e.g. say: Typical diamonds prices are about 5% ($e^{-0.054277} - 1 = -0.0528$) lower for color "E" than for color "F".

- Similarly, model errors are on log scale (and therefore approximately on approximately percentage scale). Looking at residual standard error, we can say that the typical prediction error is about 13%. 

- The R-squared tells us that about 98% of the variability in log-prices can be explained by our four covariables.

- How can we describe the effect of carat? One approach is to stay in the transformed scale: An increase in log-carat by 1 tends to a 1.88 increase in log-price. Due to the logs, this is not very easy to understand. It is easier to switch to the original scale and talk about percentages: A 1% increase in size is associated with a typical price increase of 1.9%.

## Exercises

1. Let's improve the simple linear regression on diamond prices by adding "carat squared" as additional regressor.
    - How does the resulting *quadratic* model equation look in clean math?
    - Compare the following aspects with the original simple linear regression: residual standard error, R-squared, prediction of first observation, the coefficients.
  
2. As alternative to the multiple linear regression on diamond prices with logarithmic price and logarithmic carat, consider the same model without logarithms. Interpret the output of the model.

# Generalized Linear Model

The basic linear regression model has many extensions, e.g.

- quantile regression to model quantiles of the response instead of its expectation,
- mixed-models to capture grouped data structures,
- generlized least-squares to model time series data,
- penalized regression, an extention to fight overfitting (LASSO, Ridge-Regression, Elastic-Net),
- neural networks that automatically learn interactions and non-linearities (see later),
- **the generalized linear model (GLM)** that e.g. allows to model binary response variables in a natural way.

Here, we will introduce the generalized linear model, introduced in 1972 by Nelder & Wedderburn [2]. 

## Definition

The model equation of the GLM is
$$
  g(E(Y)) = \beta_0 + \beta_1 X_1 + \dots + + \beta_m X_m
$$
resp.
$$
  E(Y) = g^{-1}(\beta_0 + \beta_1 X_1 + \dots + + \beta_m X_m),
$$
where $Y$ conditional on the covariables belongs to the exponential dispersion family and the link function $g$ is a smooth transformation. 

Thus, a GLM has three components:

1. A linear function for the covariable effects, exactly as in the linear regression.
2. The link function $g$. Its purpose is to map $E(Y)$ to the scale of the linear function. Or the other way round: The inverse link $g^{-1}$ maps the linear part to the scale of the response.
3. A distribution of $Y$ conditional on the covariables.

The following table lists some of the most used GLMs.

| Regression type | Distribution | Natural link| Typical link  |Loss function      |
|-----------------|--------------|-------------|---------------|-------------------|
| Linear          | Normal       | Identity    | Identity      | Normal deviance   |
| Binary          | Binary       | logit       | logit         | Binary deviance   |      
| Count           | Poisson      | log         | log           | Poisson deviance  |
| Log-linear      | Gamma        | 1/x         | log           | Gamma deviance    |

The logit function is defined as 
$$
  \text{logit}(p) = \log (\text{odds}(p)) = \log\left(\frac{p}{1-p}\right)
$$
It maps probabilities to the real line. The inverse logit ("sigmoidal transformation") reverts this: It maps real values to the range from 0 to 1. In the context of GLMs, $p = \text{Prob}(Y = 1) = E(Y)$.

**Comments regarding above table**

- The first type (linear) corresponds to the normal linear model.
- The second type (binary) is the famous logistic regression for binary (0/1) responses.
- The normal, Poisson and Gamma GLMs are special cases of the Tweedie GLM.
- To find predictions $\hat y$ on the scale of $Y$, one evaluates the linear function and then applies the inverse link $g^{-1}$.
- Any smooth and monotone transformation can be used as link $g$. However, only the *natural* or *canonical* link has the relevant property of providing unbiased predictions on the scale of $Y$. Thus, one usually works with the natural link with the notable exeption of the Gamma GLM, which is often applied with the log link.
- Parameters of a GLM are estimated to minimize a distribution specific criterion called *deviance* derived from maximum likelihood estimation. For the normal linear model, this criterion equals the MSE. Thus, the deviance plays the same role as the MSE for linear regression.

## Why do we need GLMs?

The normal linear model allows us to model $E(Y)$ by an additive linear function. In principle, this would also work for 

- binary $Y$ (e.g. insurance claim yes/no, success yes/no, fraud yes/no), 
- count $Y$ (e.g. number of insurance claims, number of successes),
- right-skewed $Y$ (e.g. time durations, claim heights, house prices).

However, in such cases, an additive linear model equation is usually not realistic. As such, the main assumption of the linear regression are violated. 

Here two examples:

1. Covariables often act on counts response in a multiplicative manner

- Binary: A jump from 0.5 to 0.6 success probability is less impressive than from 0.8 to 0.9. 
- Count: A jump from 2 to 3 is less impressive than a jump from 0.1 to 1.1. 
- Right-skewed: Also here, a jump from 1 Mio to 1.1 Mio is conceived as larger than the jump from 2 Mio to 2.1 Mio.

The main advantage of a GLM is...

Additionally, thanks to the inverse link, predictions on the scale of $Y$ respect the range of $Y$. E.g. probabilities predicted by a logistic regression are always between 0 an 1. Another technical advantage is that standard errors, hypothesis tests etc. are approximately correct even if the error variance is non-constant.

## Interpretation of effects

The interpretation of effects on the linear scale or for the identity link is as with linear regression: "A one-point increase in $X_m$ is associated with a $\beta_m$ increase in $g(E(y))$, keeping everything else fixed". 

Often, we want to be able to interpret the effects on $E(Y)$, i.e. with the equivalent model equation
$$
 E(y) = g^{-1}(X\beta)
$$
in mind. FOcus on log.

## Example: Claim counts

We now model claim counts for the `insuranceData` by a Poisson GLM with its natural link function, the log. It makes sure that we can interpret covariable effects on a relative scale and that predictions are positive.

```{r}
library(ggplot2)
library(insuranceData)
data(dataCar)

summary(dataCar)

# Distribution of the claim count
ggplot(dataCar, aes(x = numclaims)) +
  geom_bar(fill = "orange") +
  ggtitle("Distribution of 'numclaims'")

fit <- glm(numclaims ~ veh_value + veh_body + veh_age + gender + area + agecat,
           data = dataCar, family = poisson(link = "log"))
summary(fit)

```

**Comments**

- An increase of 1 in `veh_value` increases the log of the expected count by 0.046. On the original scale, this is an increase of $e^{0.046388}-1 = 0.047 = 4.7\%$.
- Males produce, on average, about 1.6% less claims. 
- The deviance 26629 is only 0.5% smaller than the one of the null model 26768. The predictive performance of this model is thus low. This makes sense as having claims is a highly random event that cannot be predicted on individual scale.

## Example: Claim (yes/no)

In order to illustrate logistic regression, we will model the binary variable "claim yes=1/no=0" by a logistic regression with natural logit link. This ensures that predicted probabilities are between 0 and 1 and that covariables act in a multiplicative way on the odds of having a claim.

```{r}
library(ggplot2)
library(insuranceData)
data(dataCar)

# Distribution of the claim count
ggplot(dataCar, aes(x = factor(clm))) +
  geom_bar(fill = "orange") +
  ggtitle("Distribution of 'clm'")

fit <- glm(clm ~ veh_value + veh_body + veh_age + gender + area + agecat,
           data = dataCar, family = binomial(link = "logit"))
summary(fit)

```

**Comments**

- An increase of 1 in `veh_value` (10'000 USD) increases the log-odds of having a claim by 0.051. On the odds scale, this is an increase of $e^{0.05098}-1 = 0.052 = 5.2\%$. Thus we can say that for every additional 10'000 USD vehicle value, the odds of having a claim is increased by 5.2%.
- The odds that a male produces a claim is about 1% smaller than for females.
- The deviance 33632 is only 0.4% smaller than the one of the null model 33767.

# Chapter References

[1] A. Legendre, "Nouvelles méthodes pour la détermination des orbites des comètes", 1805.

[2] J. A. Nelder and R. W. M. Wedderburn, "Generalized Linear Models". Journal of the Royal Statistical Society. Series A, Vol. 135, No. 3 (1972).
