---
title: "Model Selection"
subtitle: "WORK IN PROGRESS"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

Modeling means deciding:

- What covariables to include in the model?
- How to represent a covariable best in a linear model? With logarithms, with interactions, non-linear terms?
- Is a linear model suitable or do we need an other algorithm like nearest-neighbour, gradient boosting or a neural net?
- Are there parameters to choose? E.g. the learning rate for a neural net or the size of a decision tree?

Some of these decisions can be taken purely knowledge based (no, we don't use haircolor to predict claim amount), other require model comparison. To do this is in a fair manner is the purpose of this chapter.

## An alternative to linear models: k-nearest-neighbour

A very simple and intuitive alternative to a linear model is the k-nearest-neighbour approach. It can be applied for both regression and classification tasks and works without fitting anything. The prediction of an observation is obtained by 

1. searching the closest $k\le 1$ neighbours in the data set and then 
2. aggregating their response variable (mean or weighted mean for regression tasks, most frequent value or probabilities for classification tasks). 

By "nearest" we usually mean Euclidean distance in the covariable space. If covariables are not on the same scale, it makes sense to *standardize* them first by subtracting the mean and dividing by the standard deviation. Categorical features need to be one-hot-encoded or integer-encoded first. Note that one-hot-encoded covariables may or may not be standardized.

### Example: diamonds

What prediction would we get with five-nearest-neighbour regression for observation 10'000?
```{r}
library(ggplot2)
library(FNN)

x <- c("carat", "color", "cut", "clarity")
full <- data.matrix(diamonds[, x])
scaled <- scale(full)
apply(scaled, 2, mean)
apply(scaled, 2, sd)

# The 10'000th observation
diamonds[10000, c("price", x)]

# Its prediction
knn.reg(scaled, test = scaled[10000, ], k = 5, y = diamonds$price)

# Its five nearest neighbours
neighbours <- knn.index(scaled, k = 5)[10000, ]
neighbours
diamonds[neighbours, c("price", x)]
```

**Comments** 

- The five nearest diamonds are extremely similar. Unsurprisingly, one of the five nearest neighbours is the observation of interest.
- Their average price gives us the nearest-neighbour prediction for the 10'000th diamond.

## Naive model comparison

How well does a 1-nearest-neighbour regression perform against a linear regression? Much better. Why? Because each observation would be its own nearest neighbour, leading to perfect predictions. Such seemingly excellent performance would simply be a consequence of massive overfit. Overfit makes comparisons unfair and should be accounted for. The standard approach to do so is based on data splitting techniques. 

## Data splitting

### Simple validation

With simple validation, the original data set is split into a *training* data used to fit the models and a separate *validation* data used to take decisions like those mentioned in the introduction. The validation data set consists usually of 10%-30% of data rows. A too small validation data set will lead to wrong decisions, while a too large validation data set are wasting data for model fitting.

#### Example: diamonds

Let's see what $k$ provides the best RMSE on both training and validation data using a 80%/20% split.

```{r, cache=TRUE}
library(ggplot2)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 80% for "training" and 20% for validation
ix <- partition(diamonds$price, p = c(train = 0.8, valid = 0.2), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train$price
y_valid <- valid$price

# Standarize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to validation data
X_valid <- scale(data.matrix(valid[, x]),
                 center = attr(X_train, "scaled:center"),
                 scale = attr(X_train, "scaled:scale"))

valid_rmse <- train_rmse <- numeric(20)

for (k in 1:20) {
  # Get performance on training data
  pred_train <- knn.reg(X_train, test = X_train, k = k, y = y_train)$pred
  train_rmse[k] <- rmse(y_train, pred_train)
  
  # Get performance on valid data
  pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
  valid_rmse[k] <- rmse(y_valid, pred_valid)
}

# Combine results to a table
perf <- rbind(data.frame(k = 1:20, data = "train", rmse = train_rmse),
              data.frame(k = 1:20, data = "valid", rmse = valid_rmse))
ggplot(perf, aes(x = k, y = rmse, group = data, color = data)) +
  geom_point() +
  geom_line()

```

**Comments**:

- The amount of overfitting (difference between validation and training performance) is large for small k and small for large k. This makes sense.
- Selecting k based on training data would lead to a suboptimal model.
- Based on validation data, we would choose $k=4$. It has a minimal RMSE of 602.
- The peak at $k=1$ for the training set is unexpected. It is a consequence of the many diamonds with exactly the same covariable combination.

### Cross-validation

If our training and validation data are large and training takes long, then a simple validation strategy is usually good enough. For smaller data or if training is fast, there is an alternative to simple validation which utilizes the data in a better way and takes more robust decisions. It is called **k-fold cross-validation** and works as follows:

1. Choose a relevant performance measure, e.g. MSE for regression.
2. Split the data into $k$ pieces $D = \{D_1, \dots, D_m\}$ called "folds".
3. Set aside one of the pieces ($D_j$) for validation.
4. Fit the model on all other pieces, i.e. on $D \setminus D_j$.
5. Calculate the model performance on the validation data $D_j$.
6. Repeat Steps 3-5 for a total of $k$ times, i.e. until each observation was used for validation.
7. Average the $k$ model performances to get a final estimate of the model performance.

Use the result of the last step to choose your final model. Retrain the model on the full training data.

Note: If cross-validation is fast, you can go for repeated cross-validation, where Steps 1 to 5 are repeated multiple times on different data splits in order to get an even more robust estimate of model performance.

#### Example: diamonds

We now use five-fold cross-validation on the diamonds data to find the best $k$.

```{r, cache=TRUE}
library(ggplot2)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Response and scaled covariable matrix
y <- diamonds$price
x <- c("carat", "color", "cut", "clarity")
X <- scale(data.matrix(diamonds[, x]))

# Split diamonds into folds
nfolds <- 5
folds <- create_folds(y, k = nfolds, seed = 9838, type = "basic")

cv_rmse <- numeric(20)

for (k in 1:20) {
  rmse_per_fold <- c()
  
  for (fold in folds) {
    y_train <- y[fold]
    y_valid <- y[-fold]
    
    X_train <- X[fold, ]
    X_valid <- X[-fold, ]

    pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
    rmse_valid <- rmse(y_valid, pred_valid)
    rmse_per_fold <- c(rmse_per_fold, rmse_valid)
  }
  cv_rmse[k] <- mean(rmse_per_fold)
}
  
# Combine results to a table
perf <- data.frame(k = 1:20, rmse = cv_rmse)
ggplot(perf, aes(x = k, y = rmse)) +
  geom_point() +
  geom_line() +
  ggtitle("Performance by cross-validation")

```
**Comment:** Now, $k=5$ seems to be the best choice. Its cross-validation RMSE is 619 USD.

### Test data

Usually, many decisions are made based on (cross-)validation. Each decision tends to make the resulting model look better than it is in reality. Thus, it is often unclear how well the final model is expected to perform in reality. As a solution, one often sets aside another small *test* data set just to assess the performance of the final model. A size of 10% is usually sufficient. 
It is important to look at the test data just once at the very end of the modeling process after each decision has been made.

Depending on whether one does simple validation or cross-validation, the workflow is as follows:

**Workflow A**

1. Split data into train/valid/test, e.g. by ratios 70%/20%/10%.
2. Train different models on the training data and assess their performance on the validation data. Choose the best model, retrain it on the combination of training and validation data and call it "final model".
3. Assess the performance of the final model on the test data.

**Workflow B**

1. Split data into train/test, e.g. by ratios 90%/10%.
2. Evaluate different models by k-fold cross-validation on the training data. Choose the best model, retrain it on the full training data.
3. Assess performance of the final model on the test data.

#### Example: diamonds

We will now go through Workflow B for our diamonds. This time, we will not only consider k-nearest-neighbour, but also linear regression. 

```{r, cache=TRUE}
library(ggplot2)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 90% for training and 10% for testing
ix <- partition(diamonds$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
test <- diamonds[ix$test, ]

y_train <- train$price
y_test <- test$price

# Standarize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to test data
X_test <- scale(data.matrix(test[, x]),
                center = attr(X_train, "scaled:center"),
                scale = attr(X_train, "scaled:scale"))

# Cross-validation performance of linear regression
valid_rmse <- train_rmse <- numeric(20)

for (k in 1:20) {
  # Get performance on training data
  pred_train <- knn.reg(X_train, test = X_train, k = k, y = y_train)$pred
  train_rmse[k] <- rmse(y_train, pred_train)
  
  # Get performance on valid data
  pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
  valid_rmse[k] <- rmse(y_valid, pred_valid)
}

# Combine results to a table
perf <- rbind(data.frame(k = 1:20, data = "train", rmse = train_rmse),
              data.frame(k = 1:20, data = "valid", rmse = valid_rmse))
ggplot(perf, aes(x = k, y = rmse, group = data, color = data)) +
  geom_point() +
  geom_line()
```

## How to split the data

Often, data is split randomly for validation/test and or cross-validation. If rows are independent from each other, this will lead to good results. In many cases, rows are not independent. In these cases....

### Grouped k-fold

blabla

#### Example: diamonds

### Stratified k-fold

blabla

#### Example: diamonds

## Performance metrics and loss functions



