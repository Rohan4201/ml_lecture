---
title: "Model Selection and Validation"
subtitle: "WORK IN PROGRESS"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

In the previous chapter, we have met performance measures like the RMSE or the deviance to measure how good our models are. Unfortunately, we cannot fully rely on these values due to overfitting: The more our models overfit, the less we can trust in their "insample" performance, i.e. the performance on the data used to calculate the models. *Selecting models* based on their insample performance is equally bad. Overfitting should not be rewarded!

In this chapter, we will meet ways to estimate the performance of a model in a fair way and use it to select the best model among alternatives. They are all based on data splitting techniques, where the models are evaluated on fresh data not used for model calculation. Before introducing these, we will meet a competitor to the linear model.

# k-Nearest-Neighbour

A very simple and intuitive alternative to the linear model is the k-nearest-neighbour approach, originally introduced by Evelyn Fix and J. L. Hodges in an upublished technical report in 1951. It can be applied for both regression and classification and works without fitting anything. The prediction for an observation is obtained by 

1. searching the closest k neighbours in the data set and then 
2. combining their response. 

By "nearest" we usually mean Euclidean distance in the covariable space. If covariables are not on the same scale, it makes sense to *standardize* them first by subtracting the mean and dividing by the standard deviation. Categorical features need to be one-hot-encoded or integer-encoded first. Note that one-hot-encoded covariables may or may not be standardized.

For regression tasks, responses are often combined by computing the arithmetic mean. For classification tasks, the responses of the k neighbours are condensed by their most frequent value (the mode) or class probabilities.

## Example: diamonds

What prediction would we get with 5-nearest-neighbour regression for the 10'000th row?

```{r}
library(ggplot2)
library(FNN)

# Covariable matrix
x <- c("carat", "color", "cut", "clarity")
X <- scale(data.matrix(diamonds[, x]))

# The 10'000th observation
diamonds[10000, c("price", x)]

# Its prediction
knn.reg(X, test = X[10000, ], k = 5, y = diamonds$price)

# Its five nearest neighbours
neighbours <- knn.index(X, k = 5)[10000, ]
diamonds[neighbours, c("price", x)]
```

**Comments** 

- The five nearest diamonds are extremely similar. Unsurprisingly, one of the five nearest neighbours is the observation of interest itself, introducing a relevant amount of overfitting.
- The average price of these five observations gives us the nearest-neighbour prediction for the 10'000th diamond.
- Would we get better results for a different choice of the number of neighbours k?

# Data Splitting Techniques

Insample, a 1-nearest-neighbour regression predicts without errors and thus always outperforms linear regression, which is nonsensical. To correctly account for overfitting, we assess the performance of models on *fresh data not used for model calculation*.

## Simple validation

With simple validation, the original data set is split into a *training* data used to fit the models and a separate *validation* data used to evaluate model performance and to select models. Typically, 10%-30% of rows are used for validation.

We can use the validation data to compare *algorithms* (e.g. regression versus k-nearest-neighbour) and also to choose *hyperparameters* like $k$ of k-nearest-neighbour. Furthermore, the performance difference between training and validation data indicates the amount of overfitting.

### Example: diamonds

What $k$ provides the best RMSE on 20% validation data?

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 80% for "training" and 20% for validation
ix <- partition(diamonds$price, p = c(train = 0.8, valid = 0.2), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train$price
y_valid <- valid$price

# Standarize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to validation data
X_valid <- scale(data.matrix(valid[, x]),
                 center = attr(X_train, "scaled:center"),
                 scale = attr(X_train, "scaled:scale"))

# Tuning grid with different values for parameter k
paramGrid <- data.frame(train = NA, valid = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  # Performance on training data
  pred_train <- knn.reg(X_train, test = X_train, k = k, y = y_train)$pred
  paramGrid[i, "train"] <- rmse(y_train, pred_train)
  
  # Performance on valid data
  pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
  paramGrid[i, "valid"] <- rmse(y_valid, pred_valid)
}

# Plot results
pivot_longer(paramGrid, cols = -k, values_to = "RMSE", names_to = "Data") %>% 
ggplot(aes(x = k, y = RMSE, group = Data, color = Data)) +
  geom_point() +
  geom_line()
```

**Comments**:

- The amount of overfitting decreases for growing k, which makes sense.
- Selecting k based on the training data would lead to a suboptimal model.
- Based on the validation data, we would choose $k=4$. It has a minimal RMSE of 602.
- Why is the RMSE on the training data not 0 for 1-nearest-neighbour?

## Cross-Validation (CV)

If our training and validation data are large and training takes long, then a simple validation strategy is usually good enough. For smaller data or if training is fast, there is a better alternative that utilizes the data in a better way and takes more robust decisions. It is called **k-fold cross-validation** and works as follows:

1. Split the data into $k$ pieces $D = \{D_1, \dots, D_m\}$ called "folds".
2. Set aside one of the pieces ($D_j$) for validation.
3. Fit the model on all other pieces, i.e. on $D \setminus D_j$.
4. Calculate the model performance on the validation data $D_j$.
5. Repeat Steps 2-4 until each piece was used for validation once.
6. Average the $k$ model performances to get an estimate of the model performance.

Step 6 provides the *CV performance* of the model, a good basis to choose the best and final model. The final model is finally retrained on all folds.

Note: If cross-validation is fast, you can repeat the process for additional data splits. Such *repeated* cross-validation leads to even more robust results.

### Example: diamonds

We now use five-fold CV on the diamonds data to find the optimal $k$, i.e. to *tune* our nearest-neighbour approach.

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Response and scaled covariable matrix
y <- diamonds$price
X <- diamonds %>% 
  select(carat, color, cut, clarity) %>% 
  data.matrix() %>% 
  scale()

# Split diamonds into folds
nfolds <- 5
folds <- create_folds(y, k = nfolds, seed = 9838, type = "basic")

# Tuning grid with different values for parameter k
paramGrid <- data.frame(RMSE = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  scores <- c()
  
  for (fold in folds) {
    y_train <- y[fold]
    y_valid <- y[-fold]
    
    X_train <- X[fold, ]
    X_valid <- X[-fold, ]

    pred <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
    scores[length(scores) + 1] <- rmse(y_valid, pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}
 
paramGrid

ggplot(paramGrid, aes(x = k, y = RMSE)) +
  geom_point() +
  geom_line() +
  ggtitle("Performance by cross-validation")
```

**Comment:** Using 5-fold CV, $k=5$ seems to be the best choice with a RMSE of 619 USD.

## Grid search

In above example, we have systematically compared the CV-performance of k-nearest-neighbour for different values of k. This strategy to *tune* models resp. to select hyperparameters of a model is called **grid search CV**. In the next chapter, we will also see examples where combinations of multiple parameters have to be evaluated. To avoid too large grids, one often evaluates only a subset of parameter combinations or sample parameter combinations at random. Then, we speak of **randomized search CV**.

## Test data

Usually, one uses the validation data set or cross-validation to make many decisions. Each decision tends to make the resulting model look better than it is in reality. Thus, it is often unclear how well the final model is expected to perform in reality. As a solution, we can set aside a small *test* data set just to assess the performance of the final model. A size of 5%-10% is usually sufficient. 
It is important to look at the test data just once at the very end of the modeling process after each decision has been made.

Depending on whether one does simple validation or cross-validation, the ideal workflow is as follows:

**Workflow A**

1. Split data into train/valid/test, e.g. by ratios 70%/20%/10%.
2. Train and tune different models on the training data and assess their performance on the validation data. Choose the best model, retrain it on the combination of training and validation data and call it "final model".
3. Assess the performance of the final model on the test data.

**Workflow B**

1. Split data into train/test, e.g. by ratios 90%/10%.
2. Evaluate and tune different models by k-fold cross-validation on the training data. Choose the best model, retrain it on the full training data.
3. Assess performance of the final model on the test data.

The only difference across the two workflows is whether to use simple validation or cross-validation for making decisions.

### Example: diamonds

We will now go through Workflow B for our diamond price model. We will (1) tune the "k" of our nearest-neighbour regression and (2) compete with a linear regression.

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 90% for training and 10% for testing
ix <- partition(diamonds$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
test <- diamonds[ix$test, ]

y_train <- train$price
y_test <- test$price

# Standardize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to test data
X_test <- scale(data.matrix(test[, x]),
                center = attr(X_train, "scaled:center"),
                scale = attr(X_train, "scaled:scale"))

# Split training data into folds
nfolds <- 5
folds <- create_folds(y_train, k = nfolds, seed = 9838, type = "basic")

# Cross-validation performance of k-nearest-neighbour for k = 1-20
paramGrid <- data.frame(RMSE = NA, k = 1:20)

for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  scores <- c()
  
  for (fold in folds) {
    pred <- knn.reg(X[fold, ], test = X[-fold, ], k = k, y = y[fold])$pred
    scores[length(scores) + 1] <- rmse(y[-fold], pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}
paramGrid[order(paramGrid$RMSE)[1], ]

# Cross-validation performance of linear regression
rmse_reg <- c()

for (fold in folds) {
  fit <- lm(price ~ log(carat) + color + cut + carat, data = train[fold, ])
  pred <- predict(fit, newdata = train[-fold, ])
  rmse_reg[length(rmse_reg) + 1] <- rmse(y_train[-fold], pred)
}
(rmse_reg <- mean(rmse_reg))

# The overall best model is 6-nearest-neighbour
pred <- knn.reg(X_train, test = X_test, k = 6, y = y_train)$pred

# Test performance for the best model
rmse(y_test, pred)
```

**Comments** 

- 6-nearest-neighbour regression performs much better than linear regression.
- Its performance on the independent test data well corresponds to cross-validation performance. This is a sign that our cross-validation strategy did not introduce a lot of overfitting.

## Random splitting?

The whole point of data splitting techniques is to generate *independent data sets*, so that model performance can be estimated in a fair way. If rows are independent, then random splitting is fine. 


So far, we have split rows randomly into training/validation/test partitions or into cross-validation folds. Since data rows were independent for the diamonds data, 

Validation and cross-validation provide good estimates of "true performance" as long as rows are independent

In our examples above, we have used random splitting to create data partitions and cross-validation folds. In this section, we will mention three alternatives. They 

### Non-independent rows




Random splitting is only valid if the rows are independent. As soon as rows show additional structure such as clusters (multiple rows belonging to the same object) or even a time series structure, then special techniques have to be applied.

- **Grouped splitting:** For grouped/clustered data, it is essential to not "destroy" clusters. Grouped splitting samples *clusters* instead of rows into train/valid/test and/or cross-validation folds. Ignoring this problem is one of the most frequent reasons for too optimistic cross-validation or test performance. As such, it is also responsible for selecting suboptimal models.
- **Time series splitting:** If the data set represents time series data, the data is split in a way that makes sure the validation fold is after the training fold.

If rows are independent, random splitting is okay. If the response is unbalanced (e.g. binary with a rare category) or a numeric variable with outliers, or if there is an extremely important covariable, then **stratified splitting* is recommended to produce folds/splits with similar distribution of that variable.

To summarize: *If the data shows cluster or time series structure, use grouped splitting. Otherwise, use stratified splitting by the response or a key feature.*

### Example: diamonds

In the diamonds data, the diamonds are most probably independent from each other, thus random splitting is fine. Stratified splitting by the response `price` or the key feature `carat` is recommended.

# Performance Measures




