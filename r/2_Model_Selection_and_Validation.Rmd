---
title: "Model Selection and Validation"
subtitle: "WORK IN PROGRESS"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

In the previous chapter, we have met performance measures like the RMSE or the deviance to measure how good our models are. Unfortunately, we cannot fully rely on these values due to overfitting: The more our models overfit, the less we can trust in their "insample" performance, i.e. the performance on the data used to calculate the models. *Selecting models* based on their insample performance is equally bad. Overfitting should not be rewarded!

In this chapter, we will meet ways to estimate the performance of a model in a fair way and use it to select the best model among alternatives. They are all based on data splitting techniques, where the models are evaluated on fresh data not used for model calculation. Before introducing these techniques, we will meet a competitor of the linear model.

# k-Nearest-Neighbour

A very simple and intuitive alternative to the linear model is the k-nearest-neighbour approach, originally introduced by Evelyn Fix and J. L. Hodges in an upublished technical report in 1951. It can be applied for both regression and classification and works without fitting anything. The prediction for an observation is obtained by 

1. searching the closest k neighbours in the data set and then 
2. combining their response. 

By "nearest" we usually mean Euclidean distance in the covariable space. If covariables are not on the same scale, it makes sense to *standardize* them first by subtracting the mean and dividing by the standard deviation. Categorical features need to be one-hot-encoded or integer-encoded first. Note that one-hot-encoded covariables may or may not be standardized.

For regression tasks, responses are often combined by computing the arithmetic mean. For classification tasks, the responses of the k neighbours are condensed by their most frequent value (the mode) or class probabilities.

## Example: diamonds

What prediction would we get with 5-nearest-neighbour regression for the 10'000th row?

```{r}
library(ggplot2)
library(FNN)

# Covariable matrix
x <- c("carat", "color", "cut", "clarity")
X <- scale(data.matrix(diamonds[, x]))

# The 10'000th observation
diamonds[10000, c("price", x)]

# Its prediction
knn.reg(X, test = X[10000, ], k = 5, y = diamonds$price)

# Its five nearest neighbours
neighbours <- knn.index(X, k = 5)[10000, ]
diamonds[neighbours, c("price", x)]
```

**Comments** 

- The five nearest diamonds are extremely similar. Unsurprisingly, one of the five nearest neighbours is the observation of interest itself, introducing a relevant amount of overfitting.
- The average price of these five observations gives us the nearest-neighbour prediction for the 10'000th diamond.
- Would we get better results for a different choice of the number of neighbours k?

# Data Splitting Techniques

*Insample*, a 1-nearest-neighbour regression predicts without error, a consequence of massive overfitting. This extreme example indicates that insample performance is often not worth a penny. Models need to be evaluated on fresh, independent data not used for model calculation. This leads us to *simple validation*.

## Simple validation

With simple validation, the original data set is partitioned into *training* data used to calculate the models and a separate *validation* data set used to evaluate model performance and to select models. Typically, 10%-30% of rows are used for validation.

We can use the validation performance to compare *algorithms* (e.g. regression versus k-nearest-neighbour) and also to choose *hyperparameters* like $k$ of k-nearest-neighbour. Furthermore, the performance difference between training and validation data indicates the amount of overfitting.

### Example: diamonds

What $k$ provides the best RMSE on 20% validation data?

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 80% for "training" and 20% for validation
ix <- partition(diamonds$price, p = c(train = 0.8, valid = 0.2), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train$price
y_valid <- valid$price

# Standarize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to validation data
X_valid <- scale(data.matrix(valid[, x]),
                 center = attr(X_train, "scaled:center"),
                 scale = attr(X_train, "scaled:scale"))

# Tuning grid with different values for parameter k
paramGrid <- data.frame(train = NA, valid = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  # Performance on training data
  pred_train <- knn.reg(X_train, test = X_train, k = k, y = y_train)$pred
  paramGrid[i, "train"] <- rmse(y_train, pred_train)
  
  # Performance on valid data
  pred_valid <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
  paramGrid[i, "valid"] <- rmse(y_valid, pred_valid)
}

# Plot results
pivot_longer(paramGrid, cols = -k, values_to = "RMSE", names_to = "Data") %>% 
ggplot(aes(x = k, y = RMSE, group = Data, color = Data)) +
  geom_point() +
  geom_line()
```

**Comments**:

- The amount of overfitting decreases for growing k, which makes sense.
- Selecting k based on the training data would lead to a suboptimal model.
- Based on the validation data, we would choose $k=4$. It has a minimal RMSE of 602.
- Why is the RMSE on the training data not 0 for 1-nearest-neighbour?

## Cross-Validation (CV)

If our data set is large and training takes long, then a simple validation strategy is usually good enough. For smaller data or if training is fast, there is a better alternative that utilizes the data in a more economic way and takes more robust decisions. It is called **k-fold cross-validation** and works as follows:

1. Split the data into $k$ pieces $D = \{D_1, \dots, D_m\}$ called "folds".
2. Set aside one of the pieces ($D_j$) for validation.
3. Fit the model on all other pieces, i.e. on $D \setminus D_j$.
4. Calculate the model performance on the validation data $D_j$.
5. Repeat Steps 2-4 until each piece was used for validation once.
6. Average the $k$ model performances to get an estimate of the model performance.

Step 6 provides the *CV performance* of the model, a good basis to choose the best and final model. The final model is finally retrained on all folds.

Note: If cross-validation is fast, you can repeat the process for additional data splits. Such *repeated* cross-validation leads to even more robust results.

### Example: diamonds

We now use five-fold CV on the diamonds data to find the optimal $k$, i.e. to *tune* our nearest-neighbour approach.

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Response and scaled covariable matrix
y <- diamonds$price
X <- diamonds %>% 
  select(carat, color, cut, clarity) %>% 
  data.matrix() %>% 
  scale()

# Split diamonds into folds
nfolds <- 5
folds <- create_folds(y, k = nfolds, seed = 9838, type = "basic")

# Tuning grid with different values for parameter k
paramGrid <- data.frame(RMSE = NA, k = 1:20)
    
# Calculate performance for each row in the parameter grid
for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  
  scores <- c()
  
  for (fold in folds) {
    y_train <- y[fold]
    y_valid <- y[-fold]
    
    X_train <- X[fold, ]
    X_valid <- X[-fold, ]

    pred <- knn.reg(X_train, test = X_valid, k = k, y = y_train)$pred
    scores[length(scores) + 1] <- rmse(y_valid, pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}
 
paramGrid

ggplot(paramGrid, aes(x = k, y = RMSE)) +
  geom_point() +
  geom_line() +
  ggtitle("Performance by cross-validation")
```

**Comment:** Using 5-fold CV, $k=5$ seems to be the best choice with a RMSE of 619 USD.

## Grid search

In above example, we have systematically compared the CV-performance of k-nearest-neighbour for different values of k. This strategy to *tune* models resp. to select hyperparameters of a model is called **grid search CV**. In the next chapter, we will also see examples where combinations of multiple parameters have to be evaluated. To avoid too large grids, one often evaluates only a subset of parameter combinations or samples parameter combinations at random. Then, we speak of **randomized search CV**.

## Test data

Often, modeling involves many decisions. Even if guided by (cross-)validation, each decision tends to make the resulting final model look better than it is, an effect that can be called *overfitting on the validation data*. As a consequence, we often do not know how well the final model will perform in reality. As a solution, we can set aside a small *test* data set used to assess the performance of the *final* model. A size of 5%-10% is usually sufficient. 
It is important to look at the test data just once at the very end of the modeling process - after each decision has been made.

Depending on whether one does simple validation or cross-validation, the workflow is thus as follows:

**Workflow A**

1. Split data into train/valid/test, e.g. by ratios 70%/20%/10%.
2. Train and tune different models on the training data and assess their performance on the validation data. Choose the best model, retrain it on the combination of training and validation data and call it "final model".
3. Assess the performance of the final model on the test data.

**Workflow B**

1. Split data into train/test, e.g. by ratios 90%/10%.
2. Evaluate and tune different models by k-fold cross-validation on the training data. Choose the best model, retrain it on the full training data.
3. Assess performance of the final model on the test data.

The only difference across the two workflows is whether to use simple validation or cross-validation for making decisions.

### Example: diamonds

We will now go through Workflow B for our diamond price model. We will (1) tune the "k" of our nearest-neighbour regression and (2) compete with a linear regression.

```{r, cache=TRUE}
library(tidyverse)
library(FNN)
library(splitTools)
library(MetricsWeighted)

# Covariables
x <- c("carat", "color", "cut", "clarity")

# Split diamonds into 90% for training and 10% for testing
ix <- partition(diamonds$price, p = c(train = 0.9, test = 0.1), seed = 9838, type = "basic")

train <- diamonds[ix$train, ]
test <- diamonds[ix$test, ]

y_train <- train$price
y_test <- test$price

# Standardize training data
X_train <- scale(data.matrix(train[, x]))

# Apply training scale to test data
X_test <- scale(data.matrix(test[, x]),
                center = attr(X_train, "scaled:center"),
                scale = attr(X_train, "scaled:scale"))

# Split training data into folds
nfolds <- 5
folds <- create_folds(y_train, k = nfolds, seed = 9838, type = "basic")

# Cross-validation performance of k-nearest-neighbour for k = 1-20
paramGrid <- data.frame(RMSE = NA, k = 1:20)

for (i in 1:nrow(paramGrid)) {
  k <- paramGrid[i, "k"]
  scores <- c()
  
  for (fold in folds) {
    pred <- knn.reg(X[fold, ], test = X[-fold, ], k = k, y = y[fold])$pred
    scores[length(scores) + 1] <- rmse(y[-fold], pred)
  }
  paramGrid[i, "RMSE"] <- mean(scores)
}
paramGrid[order(paramGrid$RMSE)[1], ]

# Cross-validation performance of linear regression
rmse_reg <- c()

for (fold in folds) {
  fit <- lm(price ~ log(carat) + color + cut + carat, data = train[fold, ])
  pred <- predict(fit, newdata = train[-fold, ])
  rmse_reg[length(rmse_reg) + 1] <- rmse(y_train[-fold], pred)
}
(rmse_reg <- mean(rmse_reg))

# The overall best model is 6-nearest-neighbour
pred <- knn.reg(X_train, test = X_test, k = 6, y = y_train)$pred

# Test performance for the best model
rmse(y_test, pred)
```

**Comments** 

- 6-nearest-neighbour regression performs much better than linear regression.
- Its performance on the independent test data well corresponds to cross-validation performance. This is a sign that our cross-validation strategy did not introduce a lot of overfitting.

## Random splitting?

The data is often randomly split into partitions or folds. As long as rows are *independent*, this leads to honest estimates of model performance. 

When rows are not independent, e.g. with time series data or grouped data, such strategy is flawed and leads to too optimistic results. **This is one of the most frequent reasons to end up with a bad model. It is essential to avoid it.**

### Time-series data

When data represents a time series, splitting is best done in a way that does not shuffle the row order, e.g. in blocks: For simple validation, e.g. the first 80% of rows could be used for training and the remaining 20% for validation. 

### Grouped data

Often, data is grouped or clustered, e.g.

- multiple rows belong to the same patient/customer or
- duplicated rows (accidental or not). 

Then, instead of distributing *rows* into partitions, we should distribute *groups* to not destroy the data structure and to get honest performance estimates. We speak of *grouped splitting* and *group k-fold CV*. 

### Stratification

*If rows are independent*, there is a variant of random splitting that often provides better results and is therefore often used by default: *stratified splitting*. With stratified splitting or *stratified k-fold CV*, rows are split to ensure approximately equal distribution of a key variable (the response or a deciding covariable) across partitions/folds.

## Exercises

Apply *stratified* splitting (using the response `price`) throughout the last example. Compare the results.

# Performance Measures

Regressions are often compared by looking at mean-squared error
$$
  \text{MSE}(y, \hat y) = \frac{1}{n} \sum_{i=1}^n (y - \hat y)^2
$$
or its root. Why not the mean *absolute error* (MAE)
$$
  \text{MAE}(y, \hat y) = \frac{1}{n} \sum_{i=1}^n |y - \hat y|
$$
or even the median absolute error? Often, even before starting with the modeling task, one should choose one single, relevant *performance metric* or *scoring function*. It will guide you safely through the modeling process, eventually leading to a good model. 

| Response        | Property    | Scoring function        | Loss function         | 
|-----------------|-------------|-------------------------|-----------------------|
| Symmetric       | E(Y)        | (R)MSE                  | MSE                   |
| Right-skewed    | E(Y)        | Gamma deviance/(R)MSE   | Gamma deviance/MSE    |   
| Count           | E(Y)        | Poisson deviance/(R)MSE | Poisson deviance/MSE  |
| Numeric         | Median(Y)   | MAE                     | MAE                   |
| Binary          | E(Y)        | log-loss*               | log-loss              | 

*Same as Bernoulli deviance or cross-entropy


