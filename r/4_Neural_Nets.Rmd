---
title: "Neural Nets"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    theme: united
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

# Introduction

In this chapter, we dive into artificial neural networks, one of the main drivers of artificial intelligence. 

Neural networks are around since many decades. The first such model might be attributed to Marvin Minsky in 1951. He called his algorithm SNARC ("stochastic neural-analog reinforcement calculator"). A milestone in the development of modern neural networks was the idea of Paul J. Werbos to fit deep neural networks by "backpropagation" in 1974 [1], an algorithm still used as per today.

Artificial neural nets are extremely versatile and powerful. They can be used to

1. **fit simple models like GLMs**,
2. **learn interactions and non-linear effects in an automatic way (like tree-based methods)**,
3. fit data much larger than RAM (e.g. images),
4. learn "online" (update the model with additional data),
5. fit multiple response variables at the same time,
6. model input of dimension higher than two (e.g. images, videos),
7. model input of *different* input dimensions (e.g. text and images),
8. fit data with sequential structure in both in- and output (e.g. a text translator),
9. model data with spatial structure (images),
10. fit models with many millions of parameters,
11. do non-linear dimension reduction.

In this chapter, we will mainly deal with the first two aspects. Since a lot of new terms are being used, we have collected a small glossary in the Section "Neural Network Slang".

# Understanding Neural Nets

To learn how and why neural networks work, we will go through three steps:

- Step 1: Linear Regression as Neural Net
- Step 2: Hidden Layers
- Step 3: Activation Functions

After this, we will be ready to build more complex models.

## Step 1: Linear Regression as Neural Net

Let us revisit the simple linear regression
$$
  E(\text{price}) = \alpha + \beta \cdot \text{carat}.
$$
In Chapter 1 we have found the solution $\hat\alpha = -2256.36$ and $\hat \beta = 7756.43$ by ordinary least-squares.

Above situation can be viewed as a neural network with

- an input layer having two nodes (`carat` and the intercept called "bias" with value 1),
- a "fully connected" (= "dense") output layer with one node (`price`). Fully connected means that each node of a layer is a linear function of all node values of the previous layer. Each linear function has parameters or *weights* that need to be estimated. In our case just $\alpha$ and $\beta$.

Visualized as a graph, the situation looks as follows.

![](../figs/nn_simple_linear.PNG)

*Part of the figures were done with this cool [webtool](http://alexlenail.me/NN-SVG/index.html).*

To gain confidence into neural nets, we first show that parameters estimated by a neural network are quite similar to the ones learned by linear least-squares. To do so, we will use Google's [TensorFlow](https://www.tensorflow.org/) with its convenient [Keras](https://keras.io/) interface. 

### Example: simple linear regression

```{r}
library(tidyverse)
library(keras)
# use_python(path to python...)

# RMSE metric needs to be defined "by hand"
metric_rmse <- custom_metric("rmse", function(y_true, y_pred) {
  sqrt(k_mean(k_square(y_true - y_pred)))
})

# Input layer: we have 1 covariable
input <- layer_input(shape = 1)

# Output layer connected to the input layer
output <- input %>%
  layer_dense(units = 1)

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
# summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 1),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
history <- nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 30,
  batch_size = 100
)

plot(history, metrics = "rmse")

unlist(get_weights(nn))

# Plot effect of carat on average price
data.frame(carat = seq(0.3, 3, by = 0.1)) %>% 
  mutate(price = predict(nn, carat)) %>% 
ggplot(aes(x = carat, y = price)) +
  geom_line() +
  geom_point()
```

**Comment:** The solution of the simple neural network is indeed quite similar to the OLS solution. 

### The optimization algorithm

Neural nets are typically fitted by **mini-batch gradient descent**, using **backpropagation** to efficiently calculate gradients. It works as follows:

1. Initiate the parameters with random values. 
2. Forward step: Use the parameters to predict all observations of a *batch*. A batch is a randomly selected subset of the full data set.
3. Backpropagation step: Change the parameters in the right direction, making the loss of the current batch smaller. This involves calculating derivatives ("gradients") of the loss function (e.g. MSE) with respect to all parameters. Backpropagation does so in a layer-per-layer fashion, making heavy use of the chain rule. 
4. Repeat Steps 2-3 until each observation appeared in a batch. This is called an *epoch*.
5. Repeat Step 4 for multiple epochs until the parameter estimates stabilize or validation performance stops improving.

Gradient descent on batches of size 1 is called "stochastic gradient descent" (SGD).

## Step 2: Hidden layers

Our first neural network above consisted of only an input and an output layer. By adding one or more *hidden* layers between in- and output, the network gains additional parameters, i.e. more flexibility. The nodes of a hidden layer can be viewed as latent variables, representing the original covariables. The nodes of a hidden layer are sometimes called *embedding* or *encoding*. The closer a layer is to the output, the better its nodes are suitable to predict the response variable. In this way, a neural network finds the right transformations and interactions of its covariables in an automatic way. The only ingredience is a large data set and a flexible enough network "architecture" (number of layers, nodes per layer). 

Neural nets with more than one hidden layer are called "deep neural nets".

We will now add a hidden layer with five nodes $v_1, \dots, v_5$ to our simple linear regression network. The architecture looks as follows:

![](../figs/nn_1_hidden.PNG)

This network has 16 parameters. How much better than our simple network with just two parameters will it be?

### Example: hidden layer

```{r}
library(tidyverse)
library(keras)
# use_python(path to python...)

# RMSE metric needs to be defined "by hand"
metric_rmse <- custom_metric("rmse", function(y_true, y_pred) {
  sqrt(k_mean(k_square(y_true - y_pred)))
})

# Input layer: we have 1 covariable
input <- layer_input(shape = 1)

# One hidden layer
output <- input %>%
  layer_dense(units = 5) %>% 
  layer_dense(units = 1)

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
# summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 1),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 30,
  batch_size = 100
)

# Plot effect of carat on average price
data.frame(carat = seq(0.3, 3, by = 0.1)) %>% 
  mutate(price = predict(nn, carat)) %>% 
ggplot(aes(x = carat, y = price)) +
  geom_line() +
  geom_point()
```
**Comment:** Oops, it seems as if the extra hidden layer had no effect. The reason is that a linear function of a linear function is still a linear function. Adding the hidden layer did not really change the model. It just added a lot of unnecessary parameters.

## Step 3: Activation Functions

The missing magic component is the so called [*activation* function](https://en.wikipedia.org/wiki/Activation_function) $\sigma$ after each layer, which transforms the values of the nodes. So far, we have implicitly used "linear activations", which - in neural network slang - is just the identity function. 

The activation of the *output* layer has the same purpose as the inverse of the link function of a corresponding GLM. It maps predictions to the scale of the response: 

- linear regression -> linear activation
- binary logistic regression -> sigmoid activation (to predict probability of "1")
- multinomial logistic regression -> softmax activation (to predict one probability per class)
- log-linear regression -> exponential activation

For other layers, the purpose of the activation function is a different one: thanks to being non-linear, it adds small amounts of interactions and non-linearity. Typical activation functions are

- the hyperbolic tangent ("S"-shaped function that maps real values to $[-1, 1]$),
- the sigmoidal function ("S"-shaped function that maps real values to $[0, 1]$),
- the **re**ctangular **l** **u**nit "ReLU" $f(x) = \text{max}(0, x)$ that sets negative values to 0.

Let us add to the last example a non-linear activation $\sigma$ after the hidden layer.

![](../figs/nn_activation.PNG)

### Example: activation functions

```{r}
library(tidyverse)
library(keras)
# use_python(path to python...)

# RMSE metric needs to be defined "by hand"
metric_rmse <- custom_metric("rmse", function(y_true, y_pred) {
  sqrt(k_mean(k_square(y_true - y_pred)))
})

# Input layer: we have 1 covariable
input <- layer_input(shape = 1)

# One hidden layer
output <- input %>%
  layer_dense(units = 5, activation = 'tanh') %>% 
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)

nn %>% compile(
  optimizer = optimizer_adam(lr = 0.2),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 50,
  batch_size = 100
)

# Plot effect of carat on average price
data.frame(carat = seq(0.3, 3, by = 0.1)) %>% 
  mutate(price = predict(nn, carat)) %>% 
ggplot(aes(x = carat, y = price)) +
  geom_line() +
  geom_point()
```

**Comment:** Adding the non-linear activation after the hidden layer has changed the model. The effect of carat is now better representing the association between carat and price.

# Practical considerations

## Validation

So far, we have naively fitted the neural networks without splitting the data for test and validation. Don't do this! Usually, one sets a small test dataset (e.g. 10% of rows) aside to assess the final model performance and a validaton dataset for model tuning. 

In order to choose the main tuning parameters, namely

- network architecture,
- activation functions,
- learning rate, 
- batch size, and
- number of epochs, 

one often uses simple validation because cross-validation often takes too much time.

## Missing values

A neural net does not accept missing values in the input. They need to be filled first. For simplicity, they are often filled by a typcial value or a value below the minimum.

## Input standardization

Stochastic gradient descent starts by random initialization of parameters. This step is optimized for standardized input. Standardization has to be done manually by either

- min/max scale the values of each input to the range -1 to 1,
- standard scale the values of each input to mean 0 and standard deviation 1, or
- use relative ranks.

Note that the scaling transformation is calculated on the training data and then applied to the validation and test data. This usually requries a couple of lines of code.

## Categorical input

There are three ways to represent categorical input variables. 

1. Binary and ordinal categoricals are best represented by integers and then treated as numeric.
2. Unordered categoricals are either one-hot-encoded (i.e. each category is represented by a binary variable) or 
3. they are represented by an input embedding. With an embedding, the categories are integer encoded and then compressed by a special *embedding layer* to a few (usually 1 or 2) dense features. This requires a more complex network architecture but saves a lot of memory and preprocessing. This approach is heavily used when the input consists of words (which is a categorical variable with thousands of levels - one level per word).

For Option 2, input standardization is not required, for Option 3 it *must* not be applied as the embedding layer expects integers.

## Callbacks

Sometimes, we want to do intervene during training, e.g.

- stop training when validation performance starts worsening,
- reduce the learning rate when the network seems to be stuck in a "plateau", or
- save the network weights between epochs.

Such monitoring tasks are added by *callbacks*. We will see them in action in the example below.

## Types of layers

So far, we have encountered only dense (= fully connected) layers and activation layers. Here some further types:

- Embedding layers to represent integer encoded unordered categoricals.
- Dropout layers to add regularization.
- Convolution and pooling layers for image data.
- Recurrent layers (long-short-term memory LSTM, gated recurrent unit GRU) for sequence data.
- Concatenation layers to combine different branches of the network (like in a directed graph).
- Flatten layers to bring higher dimensional layers to dimension 1 (e.g. for emeddings, image and text data).

## Optimizer

Pure stochastic gradient descent is rarely applied without tweaks because it tends to be stuck in local minima, especially for complex networks with non-convex objective surfaces. Modern variants are "adam", "nadam" and "RMSProp". These optimizers work usually out-of-the-box, except for the learning rate, which has to be manually chosen.

## Custom losses and metrics

Keras/TensorFlow offers many predefined loss functions and evaluation metrics. Choosing them is a crucial step, just as with tree boosting.
Using TensorFlows backend functions, one can even define own metrics (see example above for the root-mean-squared error) and loss functions (see the exercise below).

## Overfitting and regularization

Like with linear models, a model with too many parameters will overfit in an undesired amount. With about 50 to 100 observations per parameter, overfitting is usually unproblematic. (For image and text data, different rules apply). As with penalized regression or trees, there are ways to actively reduce effects of overfitting in neural nets. The two main options are

- pull the parameters of a layer towards zero by L1 and/or L2 penalties,
- add dropout layers. A dropout layer randomly sets node values of the previous layer to 0, switching them off. This is an elegant way to fight overfitting as is related to bagging.

## Choosing the architecture

How many layers and number of nodes per layer to select? For tabular data, using 1-3 hidden layers is usually enough. If we start with $m$ input variabels, the number of nodes in the first hidden layer is usually higher than $m$ and reduces for later layers. There should not be a "representational bottleneck", i.e. an early hidden layer with too few parameters. 

The number of parameters should not be too high compared to the number of rows, see "Overfitting and regularization" above. 

## Interpretation

Variable importance of covariables in neural networks can be assessed by permutation importance (how much performance is lost when shuffling column X?) or SHAP importance. Covariable effects are studies by partial or SHAP dependence plots.

# Example: diamonds

We will now fit a neural net with two hidden layers (30 and 15 nodes) and a total of 631 parameters to model diamond prices. Learning rate and batch size were chosen by validation data. The number of epochs is being chosen by an early stopping callback.

![](../figs/nn_2_hidden.PNG)

```{r}
library(tidyverse)
library(keras)
# use_python(path to python...)

# RMSE metric needs to be defined "by hand"
metric_rmse <- custom_metric("rmse", function(y_true, y_pred) {
  sqrt(k_mean(k_square(y_true - y_pred)))
})

# Response and covariables
y <- "price"
x <- c("carat", "color", "cut", "clarity")

# Split into train and validation
library(splitTools)
ix <- partition(diamonds[[y]], p = c(train = 0.8, valid = 0.2), seed = 9838)

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train[[y]]
y_valid <- valid[[y]]

X_train <- train[, x]
X_valid <- valid[, x]

# Standardize X using X_train
sc <- list(
  center = attr(scale(data.matrix(X_train)), "scaled:center"),
  scale = attr(scale(data.matrix(X_train)), "scaled:scale")
)

# Function that maps data.frame to scaled network input
prep_nn <- function(X, x = c("carat", "color", "cut", "clarity"), scaling = sc) {
  X <- data.matrix(X[, x, drop = FALSE])
  scale(X, center = scaling$center, scale = scaling$scale)
}

# Input layer: we have 4 covariables
input <- layer_input(shape = 4)

# Three hidden layers with contracting number of nodes
output <- input %>%
  layer_dense(units = 30, activation = 'relu') %>% 
  layer_dense(units = 15, activation = 'relu') %>% 
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 0.3),
  loss = 'mse',
  metrics = metric_rmse
)

# Callbacks
cb <- list(
  callback_early_stopping(patience = 20),
  callback_reduce_lr_on_plateau(patience = 5)
)
       
# Fit model - naive without validation
history <- nn %>% fit(
  x = prep_nn(X_train),
  y = y_train,
  epochs = 100,
  batch_size = 400, 
  validation_data = list(prep_nn(X_valid), y_valid),
  callbacks = cb
)

history$metrics[c("rmse", "val_rmse")] %>% 
  data.frame() %>% 
  mutate(epoch = row_number()) %>% 
  pivot_longer(cols = c("rmse", "val_rmse")) %>% 
ggplot(aes(x = epoch, y = value, group = name, color = name)) +
  geom_line(size = 1.4)

# Interpret
library(flashlight)
library(MetricsWeighted)

fl <- flashlight(
  model = nn, 
  y = "price", 
  data = diamonds[ix$valid, ], 
  label = "nn", 
  metrics = list(rmse = rmse, `R squared` = r_squared),
  predict_function = function(m, X) predict(m, prep_nn(X), batch_size = 1000)
)

# Performance on validation data
plot(light_performance(fl), fill = "orange")

# Permutation importance
plot(light_importance(fl, v = x), fill = "orange")

# Partial dependence plots
plot(light_profile(fl, v = "carat", n_bins = 40)) +
  labs(title = "Partial dependence plot for carat", y = "price")

plot(light_profile(fl, v = "clarity")) +
  labs(title = "Partial dependence plot for clarity", y = "price")

plot(light_profile(fl, v = "cut")) +
  labs(title = "Partial dependence plot for cut", y = "price")

plot(light_profile(fl, v = "color")) +
  labs(title = "Partial dependence plot for color", y = "price")
```

**Comments** 

- The model performance is comparable to the tree-based models from the last chapter. 
- The effect of `carat` looks smoother as with random forests or boosted trees.

# Example: Embeddings (optional)

Representing categorical input variables through embedding layers is extremely useful in practice. We will end this chapter with an example on how to do it with the claims data. This example also shows how flexible neural network structures are.

```{r}
library(tidyverse)
library(keras)
library(splitTools)
library(insuranceData)
data(dataCar)

# use_python(path to python...)

# Response and covariables
y <- "clm"
x_emb <- "veh_body"
x_dense <- c("veh_value", "veh_age", "gender", "area", "agecat")
x <- c(x_dense, x_emb)

# Split into train and validation
ix <- partition(dataCar[[y]], p = c(train = 0.8, valid = 0.2), seed = 9838)

train <- dataCar[ix$train, ]
valid <- dataCar[ix$valid, ]

y_train <- train[[y]]
y_valid <- valid[[y]]

X_train <- train[, x]
X_valid <- valid[, x]

# Standardize X using X_train
sc <- list(
  center = attr(scale(data.matrix(X_train[, x_dense])), "scaled:center"),
  scale = attr(scale(data.matrix(X_train[, x_dense])), "scaled:scale")
)

# Function that maps data.frame to scaled network input (a list with a dense part 
# and each embedding as separat integer vector)
prep_nn <- function(X, dense = x_dense, emb = x_emb, scaling = sc) {
  X_dense <- data.matrix(X[, dense, drop = FALSE])
  X_dense <- scale(X_dense, center = scaling$center, scale = scaling$scale)
  emb <- lapply(X[emb], function(x) as.integer(x) - 1)
  c(list(dense1 = X_dense), emb)
}

# Inputs
input_dense <- layer_input(shape = length(x_dense), name = "dense1")
input_veh_body <- layer_input(shape = 1, name = "veh_body")

# Embedding of veh_body
emb_veh_body <- input_veh_body %>% 
  layer_embedding(input_dim = nlevels(dataCar$veh_body) + 1, 
                  output_dim = 1) %>% 
  layer_flatten()

# Combine dense input and embedding
outputs <- list(input_dense, emb_veh_body) %>% 
      layer_concatenate() %>% 
      layer_dense(30, activation = "tanh") %>% 
      layer_dense(1, activation = "sigmoid")

# Input
inputs <- list(dense1 = input_dense, 
               veh_body = input_veh_body)

# Create and compile model
nn <- keras_model(inputs = inputs, outputs = outputs)
summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 0.0001),
  loss = 'binary_crossentropy'
)

# Callbacks
cb <- list(
  callback_early_stopping(patience = 20),
  callback_reduce_lr_on_plateau(patience = 5)
)
       
# Fit model - naive without validation
history <- nn %>% fit(
  x = prep_nn(X_train),
  y = y_train,
  epochs = 100,
  batch_size = 400, 
  validation_data = list(prep_nn(X_valid), y_valid),
  callbacks = cb
)

history$metrics[c("loss", "val_loss")] %>% 
  data.frame() %>% 
  mutate(epoch = row_number()) %>% 
  pivot_longer(cols = c("loss", "val_loss")) %>% 
ggplot(aes(x = epoch, y = value, group = name, color = name)) +
  geom_line(size = 1.4)

# Interpret
library(flashlight)
library(MetricsWeighted)

fl <- flashlight(
  model = nn, 
  y = y, 
  data = dataCar[ix$valid, ], 
  label = "nn", 
  metrics = list(logLoss = logLoss, `R squared` = r_squared_bernoulli),
  predict_function = function(m, X) predict(m, prep_nn(X), batch_size = 1000)
)

# Performance on validation data
plot(light_performance(fl), fill = "orange")

# Permutation importance
plot(light_importance(fl, v = x), fill = "orange")

# Partial dependence
plot(light_profile(fl, v = "veh_value", n_bins = 40)) +
  labs(title = "Partial dependence plot for veh_value", y = "price")

plot(light_profile(fl, v = "veh_body"), rotate_x = TRUE) +
  labs(title = "Partial dependence plot for veh_body", y = "price")

plot(light_profile(fl, v = "area")) +
  labs(title = "Partial dependence plot for area", y = "price")

plot(light_profile(fl, v = "agecat")) +
  labs(title = "Partial dependence plot for agecat", y = "price")
```

# Exercises

1. Fit diamond prices by gamma loss with log-link, using the custom loss function defined below. Tune the model to achieve good performance on an independent test data set. Interpret the final model.
```{r}
loss_gamma <- function(y_true, y_pred) {
  -k_log(y_true / y_pred) + y_true / y_pred
}
```
2. Study either the optional claims data example or build your own neural net, predicting claim yes/no. For simplicity, you can represent the categorical feature `veh_body` by integers.

# Neural network slang

Here, we summarize some of the neural network slang.

- Activation function: The transformation applied to the node values.
- Architecture: The layout of layers and nodes.
- Backpropagation: An efficient way to calculate gradients.
- Batch: A couple of data rows used for one mini-batch gradient descent step.
- Callback: An action during training, e.g. saving weights, reducing learning rate or stop training.
- Epoch: The process of updating the network weights by gradient descent until each observation in the training set was used once.
- Embedding/Encoding: The values of latent variables of a hidden layer, usually the last.
- Gradient descent: The basic optimization algorithm of neural networks.
- Keras: User-friendly wrapper of TensorFlow.
- Layer: Main organisational unit of a neural network.
- Learning rate: Controls the step size of gradient descent, i.e. how aggressive the network learns.
- Node: Nodes on the input layer are the covariables, nodes on the output layer the response and nodes on a hidden layer are latent variables representing the covariables for the task to predict the response.
- Optimizer: The specific variant of gradient descent.
- Stochastic gradient descent (SGD): Mini-batch gradient descent with batches of size 1.
- TensorFlow: An important implementation of neural networks.
- Weights: The parameters of a neural net.

# Chapter Summary

In this chapter, we made a glipse into the world of neural networks. Step by step we have learned how a neural network works. We have used Keras and TensorFlow to build models brick by brick.

# Closing Remarks

In this lecture, we have met many ML algorithms and principles. To really get used to them, you will need a lot of practice. [Kaggle](kaggle.com) is a great place to do so and learn from the best. 

A summary and comparison of the algorithms we have met is on [github](https://github.com/mayer79/ML_Algorithm_Comparison). Here is a screenshot as per Sept. 7, 2020: 

![](../figs/comparison_ML.PNG).

# Chapter References

[1] P.J. Werbos, "Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences", Dissertation, 1974.


