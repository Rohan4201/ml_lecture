---
title: "Neural Nets"
author: "Michael Mayer"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction

Artificial neural nets are extremely versatile and powerful. They can be used to

1. **fit simple models like GLMs**,
2. **learn interactions and transformations in an automatic way (like tree-based methods)**,
3. fit data much larger than RAM (e.g. images),
4. learn "online" (update the model with additional data),
5. fit multiple response variables at the same time,
6. model input of dimension higher than two (e.g. images, videos),
7. model input of *different* input dimensions (e.g. text and images),
8. fit data with sequential structure in both in- and output (e.g. a text translator),
9. model data with spatial structure (images), and
10. fit models with many millions of parameters.

In this chapter, we will mainly deal with the first two aspects. Since a lot of new terms are being used, we have collected a glossary in the last section.

# Basics and linear models

Let us revisit the simple linear regression
$$
  E(\text{price}) = \alpha + \beta \cdot \text{carat}.
$$
In Chapter 1 we have found the solution $\hat\alpha = -2256.36$ and $\hat \beta = 7756.43$ by ordinary least-squares.

Above situation can be viewed as a neural network with

- an input layer having two nodes (`carat` and the intercept called "bias" with value 1),
- a "fully connected" (= "dense") output layer with one node (`price`). Fully connected means that each node of a layer is a linear function of all node values of the previous layer. Each linear function has parameters or network *weights* that need to be estimated. In our case just $\alpha$ and $\beta$.

![simple linear regression](../figs/nn_simple_linear.PNG)

*Part of the figures were done with this cool [webtool](http://alexlenail.me/NN-SVG/index.html).*

We want to show that the parameters estimated by the neural network will be quite similar to the ones learned by linear least-squares. To do so, we will use Google's [TensorFlow](https://www.tensorflow.org/) with its convenient [Keras](https://keras.io/) interface. 

## Example: simple linear regression
```{r, cache=TRUE}
library(ggplot2)
library(keras)
# use_python(path to python...)

# RMSE metric needs to be defined "by hand"
metric_rmse <- custom_metric("rmse", function(y_true, y_pred) {
  sqrt(k_mean(k_square(y_true - y_pred)))
})

# input layer: we have 1 covariable
input <- layer_input(shape = 1)

# Output layer directly connects to the input layer. There are no hidden layers.
output <- input %>%
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
# summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 1),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
history <- nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 30,
  batch_size = 100
)

plot(history, metrics = "rmse")

unlist(get_weights(nn))

# Plot effect of carat on average price
x <- seq(0.3, 3, by = 0.1)
plot(x, predict(nn, x), type = "b", xlab = "carat", ylab = "price")
```

Indeed: the solution of the tiny neural network is quite similar to the OLS solution. 

# The optimization algorithm

Neural nets are fitted by **stochastic gradient descent (SGD)**. It works as follows:

1. Initiate the parameters with random values around 0. 
2. Forward step: Use the parameters to predict all observations in a *batch*. A batch is a randomly selected subset of the full data set. This makes the algorithm "stochastic".
3. Backpropagation: Change the parameters in the right direction, i.e. to make the loss on the current batch smaller. This involves calculating derivatives ("gradients") of the loss function (e.g. MSE) with respect to all parameters.
4. Repeat Steps 2-3 until one *epoch* is over, i.e. each observation was used in a batch.
5. Repeat Step 4 for multiple epochs until the parameter estimates stabilize or validation performance stops improving.

In our simple example above, the derivatives can be easily calculated. The more layers a network has (see below), the more complex it gets to calculate derivatives of the loss with respect to each weight. TensorFlow is an "auto-differentiator". It calculates all symbolic derivatives in the `compile` step.

# Hidden layers

Our first neural network above consisted of only an input and an output layer. By adding one or more *hidden* layers between in- and output, the network gains additional parameters, i.e. more flexibility. The nodes of a hidden layer can be viewed as derived variables, representing the original covariables. Their values are sometimes called *embedding* or *encoding*. The closer a layer is to the output, the better its nodes are suitable to predict the response variable. In this way, a neural network finds the right transformations and interactions of its covariables in an automatic way. The only ingredience is a large data set and a flexible enough network "architecture" (number of layers, nodes per layer). 

Neural nets with more than one hidden layer are called "deep neural nets".

We will now add a hidden layer with five nodes to our simple linear regression network. The architecture looks as follows:

![simple linear regression with hidden layer](../figs/nn_1_hidden.PNG)

This network has 16 parameters (why?). Will it be more or less accurate than the simple network with just two parameters? In order to not fall in the overfitting trap, we use a 20% validation set.

## Example: linear regression with hidden layer (I)
```{r, cache=TRUE}
# Code continued from above 

# Input layer: we have 1 covariable
input <- layer_input(shape = 1)

# One hidden layer
output <- input %>%
  layer_dense(units = 5, activation = 'linear') %>% 
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
# summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 1),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 30,
  batch_size = 100, 
  validation_data = valid
)

# Plot effect of carat on average price
x <- seq(0.3, 3, by = 0.1)
plot(x, predict(nn, x), type = "b", xlab = "carat", ylab = "price")

```
Oops, it seems as if the extra hidden layer had no effect. The reason is that a linear function of a linear function is still a linear function. Adding the hidden layer did not really change the model.

# Activation functions

The missing magic component is the so called *activation* function after each layer, which transforms the values of the nodes. So far, we have used "linear activations", which - in neural network slang - is just the identity function. 

The activation of the *output* layer has the same purpose as the inverse of the link function of a corresponding GLM. It maps predictions to the scale of the response: 

- linear regression -> linear activation
- binary logistic regression -> sigmoid activation
- multinomial logistic regression -> softmax activation
- log-linear regression -> exponential activation

After all other layers, the purpose of the activation function is a different one: thanks to being non-linear, it adds small amounts of interactions and non-linearities that can be picked up by the neural net. Typical activation functions are

- the hyperbolic tangent ("S"-shaped function that maps real values to $[-1, 1]$),
- the sigmoidal function ("S"-shaped function that maps real values to $[0, 1]$),
- the linear rectangular function "ReLU" $f(x) = \text{max}(0, x)$ that sets negative values to 0. Often used for large neural networks to save computational resources.

Let's add to the last example a non-linear activation after the hidden layer.

## Example: linear regression with hidden layer (II)
```{r, cache=TRUE}
# Input layer: we have 1 covariable
input <- layer_input(shape = 1)

# One hidden layer
output <- input %>%
  layer_dense(units = 5, activation = 'tanh') %>% 
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)

nn %>% compile(
  optimizer = optimizer_adam(lr = 0.2),
  loss = 'mse',
  metrics = metric_rmse
)

# Fit model - naive without validation
nn %>% fit(
  x = diamonds$carat,
  y = diamonds$price,
  epochs = 50,
  batch_size = 100
)

# Plot effect of carat on average price
x <- seq(0.3, 3, by = 0.1)
plot(x, predict(nn, x), type = "b", xlab = "carat", ylab = "price")

```

# Practical considerations

## Validation

So far, we have naively fitted the neural networks without splitting the data for test and validation. Don't do this! Usually, one sets a small test data (e.g. 10% of rows) aside to assess the final model performance. 

In order to choose the main tuning parameters, namely

- network architecture,
- learning rate, 
- batch size, and
- number of epochs, 

one often uses a simple validation approach as cross-validation takes too much time.

## Missing values

A neural net does not accept missing values in the input. They need to be filled first (often by a typical value or a value below the minimal non-missing value (e.g. -1)).

## Input standardization

SGD starts by random initialization of network weights. This step is optimized for standardized input. This has to be done manually by either

- min/max scale the values of each input to the range -1 to 1,
- standard scale the values of each input to mean 0 and standard deviation 1, or
- use relative ranks.

Note that the scaling transformation has to be *calculated* from the training data and then *applied* to the validation and test data. This usually requries a couple of lines of code.

## Categorical input

There are three ways to represent categorical input variables. 

1. Binary and ordinal categoricals are best represented by integers.
2. Unordered categoricals are either one-hot-encoded (i.e. each category is represented by binary variable) or
3. by an input embedding. Here, the categories are integer encoded and are compressed by a special *embedding layer* to a few (usually 1 or 2) dense features. This requires a more complex network architecture but saves a lot of memory and preprocessing. This approach is heavily used when the input consists of words (which is a categorical variable with thousands of levels - one level per word).

For Option 2, input standardization is not required, for Option 3 it *must* not be applied as the embedding layer expects integers.

## Callbacks

Sometimes, we want to intervene during training to 

- stop training when validation performance starts worsening,
- reduce the learning rate when the network seems to be stuck in a "plateau", or
- save the network weights between epochs.

Such elements are added by *callbacks*. We will see it in action in the example below.

## Types of layers

So far, we have encountered only dense (and activation) layers. Here some further types of layers:

- Embedding layers to represent integer encoded unordered categoricals,
- Dropout layers to add regularization,
- convolution and pooling layers for image data,
- recurrent layers (lstm, gru) for sequence data,
- concatenate layers to combine different branches of the network (like in a directed graph),
- flatten layers to bring higher dimensional layers to dimension 1 (e.g. for emeddings, image and text data).

## Optimizer

There are different tweaks of SGD, called "optimizers". Good choices are usually "adam", "nadam" and "RMSProp". They have different options. The one to be changed by the user is the learning rate.

## Custom losses and metrics

Using TensorFlows backend functions, one can define its own metrics (see example above for the root-mean-squared error) and loss functions.

## Overfitting and regularization

Like with linear models, the number of parameters to be fitted should be selected relative to the number of observations in order to prevent strong overfitting. Having about 50 to 100 observations per parameter is usually a good choice. (For image and text data, different rules apply). As with penalized regression or trees, there are ways to actively reduce effects of overfitting in neural nets. The two main options are

- penalize the weights of a layer by L1 and/or L2 penalties (in Keras, this is an option of a dense layer),
- add a dropout layer. A dropout layer randomly sets node values of the previous layer to 0, shutting them off. This is an elegant way to fight overfitting.

## Choosing the architecture

How many layers and number of nodes per layer to be chosen? For tabular data, using 1-3 hidden layers is usually a good choice. If we start with $m$ input variabels, the number of nodes in the first hidden layer is usually higher than $m$, e.g. $4m$ and contracts for later layers. There should not be a "representational bottleneck", i.e. an early hidden layer with too few parameters. 

The number of parameters should not be too high compared to the number of rows, see "Overfitting and regularization" above. 

## Interpretation

Variable importance of covariables in neural networks can be assessed by permutation importance (how much performance is lost when shuffling column X?) or SHAP importance. Covariable effects are studies by partial or SHAP dependence plots.

# Example: diamonds

```{r, cache=TRUE}
# Response and covariables
y <- "price"
x <- c("carat", "color", "cut", "clarity")

# Split into train and validation
library(splitTools)
ix <- partition(diamonds[[y]], p = c(train = 0.8, valid = 0.2), seed = 9838)

train <- diamonds[ix$train, ]
valid <- diamonds[ix$valid, ]

y_train <- train[[y]]
y_valid <- valid[[y]]

X_train <- train[, x]
X_valid <- valid[, x]

# Standardize X using X_train
sc <- list()
sc$center <- attr(scale(data.matrix(X_train)), "scaled:center")
sc$scale <- attr(scale(data.matrix(X_train)), "scaled:scale")

# Function that maps data.frame to scaled network input
prep_nn <- function(X, x = c("carat", "color", "cut", "clarity"), scaling = sc) {
  X <- data.matrix(X[, x, drop = FALSE])
  scale(X, center = scaling$center, scale = scaling$scale)
}

# Input layer: we have 4 covariables
input <- layer_input(shape = 4)

# Three hidden layers with contracting number of nodes
output <- input %>%
  layer_dense(units = 20, activation = 'tanh') %>% 
  layer_dense(units = 10, activation = 'tanh') %>% 
  layer_dense(units = 5, activation = 'tanh') %>% 
  layer_dense(units = 1, activation = 'linear')

# Create and compile model
nn <- keras_model(inputs = input, outputs = output)
summary(nn)
nn %>% compile(
  optimizer = optimizer_adam(lr = 0.2),
  loss = 'mse'
)

# Callbacks
cb <- list(
  callback_early_stopping(patience = 10),
  callback_reduce_lr_on_plateau(patience = 5)
)
       
# Fit model - naive without validation
history <- nn %>% fit(
  x = prep_nn(X_train),
  y = y_train,
  epochs = 200,
  batch_size = 1000, 
  validation_data = list(prep_nn(X_valid), y_valid),
  callbacks = cb
)

# Interpret
library(flashlight)
library(MetricsWeighted)

fl <- flashlight(model = nn, 
                 y = "price", 
                 data = diamonds[ix$valid, ], 
                 label = "nn", 
                 metrics = list(rmse = rmse, `R squared` = r_squared),
                 predict_function = function(m, X) predict(m, prep_nn(X), batch_size = 1000))

# Performance on validation data
plot(light_performance(fl), fill = "orange")

# Permutation importance
plot(light_importance(fl, v = x), fill = "orange")

# Partial dependence plots
plot(light_profile(fl, v = "carat", n_bins = 40)) +
  labs(title = "Partial dependence plot for carat", y = "price")

plot(light_profile(fl, v = "clarity")) +
  labs(title = "Partial dependence plot for clarity", y = "price")

plot(light_profile(fl, v = "cut")) +
  labs(title = "Partial dependence plot for cut", y = "price")

plot(light_profile(fl, v = "color")) +
  labs(title = "Partial dependence plot for color", y = "price")

```

# Neural network slang

- Activation (function): The transformation applied on the node values.
- Architecture: The layout of layers and nodes per layer.
- Batch: A couple of rows of the full training data used for one stochastic gradient descent update.
- Callback: An action to be taken depending during training, e.g. saving weights, reducing learning rate or stop training.
- Epoch: The process of updating the network weights by stochastic gradient descent until each observation in the training set was used once.
- Embedding/Encoding: The node values (e.g. of the last hidden layer).
- Keras: User-friendly layer over TensorFlow.
- Layer: Main organization unit of a neural network.
- Learning rate: Controls the step size of SGD, i.e. how aggressive the network learns.
- Node: Nodes on the input layer are the covariables, nodes on the output layer the response and nodes on a hidden layer are latent variables representing the covariables for the task to predict the response.
- Optimizer: The specific variant of SGD.
- Stochastic gradient descent (SGD): The optimization algorithm of standard neural networks.
- TensorFlow: An important implementation of neural networks.
- Weights: The parameters of a neural net.